[base]
package = ocean
env_name = puffer_codeball
policy_name = Policy
; policy_name = MLPPolicy
rnn_name = Recurrent

[rnn]
; input_size = 256
; hidden_size = 256
; num_layers = 3

[env]
num_envs = 32
n_robots = 8
n_nitros = 4
frame_skip = 5
max_steps = 1500
scripted_opponent_type = zero
goal_scored_reward = 10.0
loiter_penalty = 0.0
ball_reward = 0.0

[train]
total_timesteps = 200_000_000
num_envs = 2
num_workers = 2
; env_batch_size = 1
batch_size = 65536
update_epochs = 1
; minibatch_size = 32768
bptt_horizon = 16
anneal_lr = False
; gae_lambda = 0.9776227170639571
; gamma = 0.8567482546637853
; clip_coef = 0.011102333784435113
; vf_coef = 0.3403069830175013
; vf_clip_coef = 0.26475190539131727
max_grad_norm = 0.8660179376602173
; ent_coef = 0.01376980586465873
learning_rate = 0.004064722899262613
; learning_rate = 0.0005978428084749276
minibatch_size = 4096
; bptt_horizon = 16
; anneal_lr = False

; gamma = 0.5
; gamma = 0.95
; gamma = 0.95
gamma = 0.998
; gamma = 0.0
; gae_lambda = 0.98
gae_lambda = 0.98

; gamma = 0.9257755108746066
; gae_lambda = 0.8783667470139129

; gamma = 0.98
; gae_lambda = 0.97
; ent_coef = 0.0012080029654114927
ent_coef = 0.0001
; max_grad_norm = 0.3808319568634033
vf_coef = 0.5
checkpoint_interval = 20
; device = cuda
device = cpu

[sweep.metric]
goal = maximize
name = environment/reward
